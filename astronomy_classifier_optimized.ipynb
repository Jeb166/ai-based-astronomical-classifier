{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5454fca8",
   "metadata": {},
   "source": [
    "# Astronomi Sınıflandırıcı - Optimize Edilmiş Standart Model\n",
    "\n",
    "Bu notebook, astronomik nesneleri sınıflandırmak için optimize edilmiş bir derin öğrenme modeli içerir. Yapılan testler sonucu standart modelin en iyi performansı (%64.61 doğruluk) sağladığı belirlenmiştir ve diğer modeller kaldırılmıştır.\n",
    "\n",
    "## Özet\n",
    "- SDSS verileri kullanılarak galaksi/kuasar/yıldız sınıflandırması yapılır\n",
    "- Yıldız alt türleri (A, F, G, K, M) ayrıca sınıflandırılır\n",
    "- Optimizasyon: En iyi performans gösteren standart model kullanılır\n",
    "- Standart model performansı: ~64.61% doğruluk\n",
    "\n",
    "## Kullanım\n",
    "1. Bu notebook'u çalıştırarak modeli eğitebilirsiniz\n",
    "2. Veya önceden eğitilmiş modeli yükleyerek tahminler yapabilirsiniz\n",
    "3. Kendi verinizi kullanmak için X_test formatına uygun giriş sağlayın"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53fe1a5",
   "metadata": {},
   "source": [
    "## Kurulum ve Bağımlılıklar\n",
    "\n",
    "Gerekli tüm kütüphaneleri yükleyelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc5b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn pandas matplotlib joblib tensorflow seaborn scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler, Callback\n",
    "import joblib\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU kullanım ayarları\n",
    "print(\"TensorFlow sürümü:\", tf.__version__)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU kullanıma hazır: {len(gpus)} adet GPU bulundu\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU ayarlanırken hata: {e}\")\n",
    "else:\n",
    "    print(\"GPU bulunamadı, CPU kullanılacak.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991801bb",
   "metadata": {},
   "source": [
    "## Veri Yükleme ve Hazırlama\n",
    "\n",
    "Öncelikle verileri indirelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf7df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri klasörü oluştur\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Ana veri dosyasını indir\n",
    "skyserver_url = \"https://raw.githubusercontent.com/astronexus/astronomy-ml-dataset/master/sdss/sdss_sample_data.csv\"\n",
    "!curl -L {skyserver_url} -o data/skyserver.csv\n",
    "\n",
    "# Yıldız alt tür verisini indir (eğer varsa)\n",
    "star_url = \"https://raw.githubusercontent.com/astronexus/astronomy-ml-dataset/master/sdss/star_subtypes.csv\"\n",
    "!curl -L {star_url} -o data/star_subtypes.csv\n",
    "\n",
    "# İndirilen verileri kontrol et\n",
    "!ls -la data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b52a6d",
   "metadata": {},
   "source": [
    "### Veri Hazırlama Fonksiyonları\n",
    "\n",
    "Ana veri ve yıldız verisinin hazırlanması için gerekli fonksiyonları tanımlayalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e12bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare(data_path, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"Ana GAL/QSO/STAR sınıfları için verileri yükler ve hazırlar\"\"\"\n",
    "    # Veriyi yükle\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Yüklenen veri: {df.shape[0]} satır, {df.shape[1]} sütun\")\n",
    "    \n",
    "    # Eksik değer kontrolü\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        print(\"Eksik değerler temizleniyor...\")\n",
    "        df = df.dropna()\n",
    "        print(f\"Temizleme sonrası: {df.shape[0]} satır\")\n",
    "    \n",
    "    # Özellikler ve hedef değişken\n",
    "    X = df.drop(['class', 'objid', 'specobjid', 'ra', 'dec'], axis=1, errors='ignore').values\n",
    "    y = pd.get_dummies(df['class']).values\n",
    "    \n",
    "    # Veriyi ölçeklendir\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Eğitim ve test setlerini ayır\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Validation seti ayır\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_ratio, random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Eğitim seti: {X_train.shape[0]} örnek\")\n",
    "    print(f\"Doğrulama seti: {X_val.shape[0]} örnek\")\n",
    "    print(f\"Test seti: {X_test.shape[0]} örnek\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, df\n",
    "\n",
    "def load_star_subset(data_path, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"Yıldız alt türleri (A, F, G, K, M) için verileri yükler ve hazırlar\"\"\"\n",
    "    # Veriyi yükle\n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "        print(f\"Yıldız verisi yüklendi: {df.shape[0]} satır, {df.shape[1]} sütun\")\n",
    "    except Exception as e:\n",
    "        print(f\"Veri yüklenirken hata: {e}\")\n",
    "        print(\"Test için örnek veri oluşturuluyor...\")\n",
    "        # Örnek veri oluştur (gerçek veri yoksa)\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        n_features = 10\n",
    "        X = np.random.randn(n_samples, n_features)\n",
    "        classes = ['A', 'F', 'G', 'K', 'M']\n",
    "        y = np.random.choice(classes, size=n_samples)\n",
    "        # DataFrame oluştur\n",
    "        df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(n_features)])\n",
    "        df['subclass'] = y\n",
    "        print(f\"Örnek veri oluşturuldu: {df.shape[0]} satır, {df.shape[1]} sütun\")\n",
    "    \n",
    "    # Eksik değer kontrolü\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        print(\"Eksik değerler temizleniyor...\")\n",
    "        df = df.dropna()\n",
    "        print(f\"Temizleme sonrası: {df.shape[0]} satır\")\n",
    "    \n",
    "    # Sınıf değerlerini kodla\n",
    "    le = LabelEncoder()\n",
    "    df['subclass_id'] = le.fit_transform(df['subclass'])\n",
    "    \n",
    "    # Özellikler ve hedef değişken\n",
    "    feature_cols = [col for col in df.columns if col not in ['subclass', 'subclass_id', 'objid', 'specobjid', 'ra', 'dec']]\n",
    "    X = df[feature_cols].values\n",
    "    y = pd.get_dummies(df['subclass_id']).values\n",
    "    \n",
    "    # Veriyi ölçeklendir\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Eğitim ve test setlerini ayır\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Validation seti ayır\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_ratio, random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Eğitim seti: {X_train.shape[0]} örnek\")\n",
    "    print(f\"Doğrulama seti: {X_val.shape[0]} örnek\")\n",
    "    print(f\"Test seti: {X_test.shape[0]} örnek\")\n",
    "    print(f\"Sınıf sayısı: {y.shape[1]}\")\n",
    "    print(f\"Sınıflar: {list(le.classes_)}\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, le, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666584e3",
   "metadata": {},
   "source": [
    "## Model Tanımlamaları\n",
    "\n",
    "Optimize edilmiş standart model yapısını tanımlayalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501bc7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, n_classes, learning_rate=0.001):\n",
    "    \"\"\"Ana GAL/QSO/STAR sınıflandırıcı modelini oluşturur\"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(1e-5)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(1e-5)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_star_model(input_dim, n_classes, \n",
    "                    neurons1=256, neurons2=128, neurons3=64, \n",
    "                    dropout1=0.4, dropout2=0.4, dropout3=0.3,\n",
    "                    learning_rate=0.001, **kwargs):\n",
    "    \"\"\"Yıldız alt türlerini sınıflandırmak için modeli oluşturur\"\"\"\n",
    "    # Standart model (yüksek performanslı)\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        Dense(neurons1, activation='relu', kernel_regularizer=l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout1),\n",
    "        Dense(neurons2, activation='relu', kernel_regularizer=l2(1e-4)),\n",
    "        BatchNormalization(), \n",
    "        Dropout(dropout2),\n",
    "        Dense(neurons3, activation='relu', kernel_regularizer=l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout3),\n",
    "        Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_star_model(model, X_train, y_train, X_val, y_val, class_weights=None, \n",
    "                    max_samples=None, batch_size=128, epochs=20, \n",
    "                    use_cyclic_lr=True, use_trending_early_stop=True):\n",
    "    \"\"\"Yıldız modelini eğitmek için gelişmiş stratejiler\"\"\"\n",
    "    # Eğitim veri setini küçültme (alt örnekleme)\n",
    "    if max_samples and len(X_train) > max_samples:\n",
    "        print(f\"Eğitim veri setini {max_samples} örneğe küçültüyorum...\")\n",
    "        idx = np.random.choice(len(X_train), max_samples, replace=False)\n",
    "        X_train_sample = X_train[idx]\n",
    "        y_train_sample = y_train[idx]\n",
    "    else:\n",
    "        X_train_sample, y_train_sample = X_train, y_train\n",
    "    \n",
    "    # Callback'leri hazırla\n",
    "    callbacks = []\n",
    "    \n",
    "    # 1. Trendi izleyen erken durdurma\n",
    "    if use_trending_early_stop:\n",
    "        class TrendingEarlyStopping(Callback):\n",
    "            \"\"\"Uzun vadeli eğilimi izleyen erken durdurma\"\"\"\n",
    "            \n",
    "            def __init__(self, monitor='val_loss', patience=5, window_size=8, min_delta=0):\n",
    "                super(TrendingEarlyStopping, self).__init__()\n",
    "                self.monitor = monitor\n",
    "                self.patience = patience\n",
    "                self.window_size = window_size\n",
    "                self.min_delta = min_delta\n",
    "                self.wait = 0\n",
    "                self.best = float('inf') if 'loss' in monitor else -float('inf')\n",
    "                self.history = []\n",
    "                \n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                logs = logs or {}\n",
    "                current = logs.get(self.monitor)\n",
    "                if current is None:\n",
    "                    return\n",
    "                \n",
    "                self.history.append(current)\n",
    "                \n",
    "                if len(self.history) >= self.window_size:\n",
    "                    # Son window_size değerini kullanarak eğilimi hesapla\n",
    "                    recent = self.history[-self.window_size:]\n",
    "                    # Doğrusal regresyon eğimi\n",
    "                    x = np.arange(len(recent))\n",
    "                    slope = np.polyfit(x, recent, 1)[0]\n",
    "                    \n",
    "                    # 'loss' için negatif eğim iyidir, diğer metrikler için pozitif\n",
    "                    is_improving = slope < -self.min_delta if 'loss' in self.monitor else slope > self.min_delta\n",
    "                    \n",
    "                    if not is_improving:\n",
    "                        self.wait += 1\n",
    "                        if self.wait >= self.patience:\n",
    "                            self.model.stop_training = True\n",
    "                            print(f\"\\nEğitim durduruldu: {self.window_size} epoch'luk eğilim iyileşmiyor.\")\n",
    "                    else:\n",
    "                        self.wait = 0\n",
    "        \n",
    "        trend_stopping = TrendingEarlyStopping(\n",
    "            monitor='val_categorical_accuracy',\n",
    "            patience=3,\n",
    "            window_size=6,\n",
    "            min_delta=0.001\n",
    "        )\n",
    "        callbacks.append(trend_stopping)\n",
    "    else:\n",
    "        # Standart erken durdurma\n",
    "        callbacks.append(\n",
    "            EarlyStopping(\n",
    "                monitor='val_categorical_accuracy',\n",
    "                patience=3,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # 2. Döngüsel öğrenme oranı\n",
    "    if use_cyclic_lr:\n",
    "        def cyclic_learning_rate(epoch, min_lr=1e-5, max_lr=1e-3, cycle_length=5):\n",
    "            \"\"\"Döngüsel öğrenme oranı planı\"\"\"\n",
    "            # Döngü içindeki mevcut konum (0 ile 1 arasında)\n",
    "            cycle_progress = (epoch % cycle_length) / cycle_length\n",
    "            \n",
    "            # Kosinüs dalgası (0 ile 1 arasında, yarım döngü)\n",
    "            cos_wave = 0.5 * (1 + np.cos(np.pi * (cycle_progress * 2 - 1)))\n",
    "            \n",
    "            # Logaritmik ölçekte yumuşak geçiş\n",
    "            log_min, log_max = np.log10(min_lr), np.log10(max_lr)\n",
    "            log_lr = log_min + cos_wave * (log_max - log_min)\n",
    "            \n",
    "            return 10 ** log_lr\n",
    "        \n",
    "        lr_scheduler = LearningRateScheduler(cyclic_learning_rate)\n",
    "        callbacks.append(lr_scheduler)\n",
    "    else:\n",
    "        # Standart öğrenme oranı azaltıcı\n",
    "        callbacks.append(\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=2,\n",
    "                min_lr=1e-6,\n",
    "                verbose=1\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Eğitim\n",
    "    history = model.fit(\n",
    "        X_train_sample, y_train_sample,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9248701f",
   "metadata": {},
   "source": [
    "## Model Eğitimi ve Değerlendirme\n",
    "\n",
    "Şimdi modellerimizi standart yapılandırma ile eğitelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f6801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Çıktı dizini oluştur\n",
    "out_dir = 'outputs'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Ana verileri yükle\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANA VERİ YÜKLENİYOR & GAL/QSO/STAR MODELİ EĞİTİLİYOR\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_tr, X_val, X_te, y_tr, y_val, y_te, df_full = load_and_prepare('data/skyserver.csv')\n",
    "y_tr_lbl  = y_tr.argmax(1)\n",
    "y_val_lbl = y_val.argmax(1)\n",
    "y_te_lbl  = y_te.argmax(1)\n",
    "\n",
    "# DNN modeli eğit\n",
    "print(\"\\nDNN modeli eğitiliyor...\")\n",
    "dnn = build_model(X_tr.shape[1], y_tr.shape[1])\n",
    "dnn.fit(\n",
    "    X_tr, y_tr,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(factor=0.5, patience=3, verbose=1)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Random Forest modeli eğit\n",
    "print(\"\\nRandom Forest modeli eğitiliyor...\")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    oob_score=True)\n",
    "rf.fit(X_tr, y_tr_lbl)\n",
    "print(f\"RF OOB accuracy: {rf.oob_score_:.4f}\")\n",
    "\n",
    "# En iyi ağırlık kombinasyonunu bul\n",
    "print(\"\\nDoğrulama setinde en iyi ağırlık kombinasyonu aranıyor...\")\n",
    "dnn_val = dnn.predict(X_val)\n",
    "rf_val  = rf.predict_proba(X_val)\n",
    "best_w, best_acc = 0.5, 0.0\n",
    "for w in np.linspace(0.1, 0.9, 9):\n",
    "    if (proba_acc := ((w*dnn_val+(1-w)*rf_val).argmax(1)==y_val_lbl).mean()) > best_acc:\n",
    "        best_w, best_acc = w, proba_acc\n",
    "print(f\"En iyi DNN ağırlığı: {best_w:.2f}  (doğrulama doğruluğu={best_acc*100:.2f}%)\")\n",
    "\n",
    "# Test setinde değerlendir\n",
    "print(\"\\nTest setinde değerlendiriliyor...\")\n",
    "dnn_probs = dnn.predict(X_te)\n",
    "rf_probs  = rf.predict_proba(X_te)\n",
    "ens_probs = best_w*dnn_probs + (1-best_w)*rf_probs\n",
    "dnn_acc = (dnn_probs.argmax(1)==y_te_lbl).mean()*100\n",
    "rf_acc  = (rf_probs.argmax(1)==y_te_lbl).mean()*100\n",
    "ens_acc = (ens_probs.argmax(1)==y_te_lbl).mean()*100\n",
    "print(f\"DNN Test Doğruluğu : {dnn_acc:6.3f}%\")\n",
    "print(f\"RF Test Doğruluğu  : {rf_acc :6.3f}%\")\n",
    "print(f\"ENS Test Doğruluğu : {ens_acc:6.3f}%\")\n",
    "\n",
    "# Confusion matrix görselleştir\n",
    "labels = np.array(['GALAXY', 'QSO', 'STAR'])  # Sınıf etiketleri\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_te_lbl, ens_probs.argmax(1))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix — Ensemble Model (DNN + RF)')\n",
    "plt.ylabel('Gerçek Sınıf')\n",
    "plt.xlabel('Tahmin Edilen Sınıf')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{out_dir}/confusion_ens.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Modelleri kaydet\n",
    "dnn.save(f\"{out_dir}/dnn_model.keras\")\n",
    "joblib.dump(rf, f\"{out_dir}/rf_model.joblib\")\n",
    "print(f\"Ana sınıflandırıcı modelleri kaydedildi: {out_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f9b166",
   "metadata": {},
   "source": [
    "## Yıldız Alt Tür Modeli Eğitimi\n",
    "\n",
    "Şimdi yıldız alt türlerini sınıflandırmak için optimizasyonu yapılmış standart modeli eğitelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yıldız verisini yükle\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"YILDIZ ALT TÜR MODELİ EĞİTİMİ\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "Xs_tr, Xs_val, Xs_te, ys_tr, ys_val, ys_te, le_star, scaler_star = load_star_subset('data/star_subtypes.csv')\n",
    "\n",
    "# Sınıf ağırlıklarını hesapla\n",
    "y_int = ys_tr.argmax(1)\n",
    "cw = class_weight.compute_class_weight(\"balanced\", classes=np.unique(y_int), y=y_int)\n",
    "cw_dict = dict(enumerate(cw))\n",
    "\n",
    "# Standart model eğitimi\n",
    "print(\"\\nStandart model eğitiliyor...\")\n",
    "n_features = Xs_tr.shape[1]\n",
    "n_classes = ys_tr.shape[1]\n",
    "\n",
    "# Modeli oluştur\n",
    "star_net = build_star_model(\n",
    "    n_features, n_classes,\n",
    "    neurons1=256,\n",
    "    neurons2=128,\n",
    "    neurons3=64,\n",
    "    dropout1=0.3,\n",
    "    dropout2=0.3,\n",
    "    dropout3=0.3,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Modeli eğit\n",
    "star_net, history = train_star_model(\n",
    "    star_net, Xs_tr, ys_tr, Xs_val, ys_val, \n",
    "    class_weights=cw_dict,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    use_cyclic_lr=True,\n",
    "    use_trending_early_stop=True\n",
    ")\n",
    "\n",
    "# Test doğruluğunu değerlendir\n",
    "preds = star_net.predict(Xs_te)\n",
    "star_acc = (preds.argmax(1) == ys_te.argmax(1)).mean() * 100\n",
    "print(f\"\\nYıldız alt türleri test doğruluğu: {star_acc:.2f}%\")\n",
    "\n",
    "# Hata metriklerini yazdır\n",
    "print(\"\\nAyrıntılı sınıflandırma raporu:\")\n",
    "y_true = ys_te.argmax(1)\n",
    "y_pred = preds.argmax(1)\n",
    "print(classification_report(y_true, y_pred, target_names=le_star.classes_))\n",
    "\n",
    "# Confusion matrix'i görselleştir\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=le_star.classes_, yticklabels=le_star.classes_)\n",
    "plt.title('Confusion Matrix — Yıldız Alt Türleri')\n",
    "plt.ylabel('Gerçek Sınıf')\n",
    "plt.xlabel('Tahmin Edilen Sınıf')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{out_dir}/confusion_star.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Modeli kaydet\n",
    "star_net.save(f\"{out_dir}/star_model.keras\")\n",
    "joblib.dump(le_star, f\"{out_dir}/star_label_enc.joblib\")\n",
    "joblib.dump(scaler_star, f\"{out_dir}/star_scaler.joblib\")\n",
    "print(f\"Yıldız alt tür modeli kaydedildi: {out_dir}/star_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799df75",
   "metadata": {},
   "source": [
    "## Tahmin Fonksiyonları ve Kullanıcı Arayüzü\n",
    "\n",
    "Eğitilmiş modelleri kullanarak tahmin yapacak fonksiyonları tanımlayalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33bcbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_dir='outputs'):\n",
    "    \"\"\"Kaydedilmiş modelleri yükle\"\"\"\n",
    "    try:\n",
    "        # Ana modeller\n",
    "        dnn = load_model(f\"{model_dir}/dnn_model.keras\")\n",
    "        rf = joblib.load(f\"{model_dir}/rf_model.joblib\")\n",
    "        \n",
    "        # Yıldız modeli ve gerekli dönüşümler\n",
    "        star_net = load_model(f\"{model_dir}/star_model.keras\")\n",
    "        le_star = joblib.load(f\"{model_dir}/star_label_enc.joblib\")\n",
    "        scaler_star = joblib.load(f\"{model_dir}/star_scaler.joblib\")\n",
    "        \n",
    "        # En iyi ağırlık (varsayılan değer)\n",
    "        best_w = 0.7\n",
    "        \n",
    "        print(\"Tüm modeller başarıyla yüklendi!\")\n",
    "        return dnn, rf, star_net, le_star, scaler_star, best_w\n",
    "    except Exception as e:\n",
    "        print(f\"Model yüklenirken hata oluştu: {e}\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "def full_predict(sample_array, dnn, rf, star_net, le_star, scaler_star, best_w=0.7):\n",
    "    \"\"\"Tüm tahmin sistemini çalıştır: GALAXY/QSO/STAR-alt tür\"\"\"\n",
    "    # Ana sınıf tahmini (GAL/QSO/STAR)\n",
    "    dnn_pred = dnn.predict(sample_array)\n",
    "    rf_pred = rf.predict_proba(sample_array)\n",
    "    ensemble_pred = best_w * dnn_pred + (1-best_w) * rf_pred\n",
    "    primary_class = ensemble_pred.argmax(1)\n",
    "    \n",
    "    # Sınıf etiketleri \n",
    "    labels = np.array(['GALAXY', 'QSO', 'STAR'])  # Sınıf etiketleri\n",
    "    STAR_ID = 2  # STAR sınıfının indeksi (genellikle 2)\n",
    "    \n",
    "    # Sonuçları formatla\n",
    "    results = []\n",
    "    for i, cls in enumerate(primary_class):\n",
    "        if cls == STAR_ID:\n",
    "            # Yıldız alt türünü tahmin et\n",
    "            scaled_sample = scaler_star.transform(sample_array[i:i+1])\n",
    "            star_subtype_id = star_net.predict(scaled_sample).argmax(1)[0]\n",
    "            star_subtype = le_star.inverse_transform([star_subtype_id])[0]\n",
    "            results.append(f\"STAR-{star_subtype}\")\n",
    "        else:\n",
    "            results.append(labels[cls])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_sample_data(n_samples=5):\n",
    "    \"\"\"Örnek tahmin için veri hazırla\"\"\"\n",
    "    try:\n",
    "        # Test verisinden örnek al\n",
    "        if 'X_te' in globals():\n",
    "            samples = X_te[:n_samples]\n",
    "            true_labels = labels[y_te[:n_samples].argmax(1)]\n",
    "            return samples, true_labels\n",
    "        else:\n",
    "            # Test verisi yoksa rastgele veri oluştur\n",
    "            print(\"Test verisi bulunamadı, rastgele örnekler oluşturuluyor...\")\n",
    "            return np.random.randn(n_samples, 10), [\"Bilinmiyor\"] * n_samples\n",
    "    except Exception as e:\n",
    "        print(f\"Örnek veri hazırlanırken hata: {e}\")\n",
    "        return np.random.randn(n_samples, 10), [\"Bilinmiyor\"] * n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc055d",
   "metadata": {},
   "source": [
    "## Örnek Tahminler\n",
    "\n",
    "Eğitilen modeli kullanarak tahminler yapalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b933978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eğitilmiş modelleri yükle\n",
    "dnn, rf, star_net, le_star, scaler_star, best_w = load_models()\n",
    "\n",
    "if dnn is not None:\n",
    "    # Örnek verileri al\n",
    "    samples, true_labels = get_sample_data(10)\n",
    "    \n",
    "    # Tahmin yap\n",
    "    predictions = full_predict(samples, dnn, rf, star_net, le_star, scaler_star, best_w)\n",
    "    \n",
    "    # Sonuçları göster\n",
    "    print(\"\\n=== ÖRNEK TAHMİNLER ===\")\n",
    "    for i, (pred, true) in enumerate(zip(predictions, true_labels)):\n",
    "        print(f\"Örnek {i+1}: Tahmin = {pred}, Gerçek = {true}\")\n",
    "else:\n",
    "    print(\"Modeller yüklenemediği için tahmin yapılamıyor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e23683",
   "metadata": {},
   "source": [
    "## Özet ve Sonuç\n",
    "\n",
    "Bu projede astronomik nesneleri (galaksi, kuasar, yıldız) sınıflandıran ve yıldızların alt türlerini (A, F, G, K, M) tahmin eden bir derin öğrenme sistemi geliştirdik.\n",
    "\n",
    "### Önemli Noktalar\n",
    "- Optimizasyon sonucu **standart model mimarisi** en iyi performansı (%64.61 doğruluk) gösterdi\n",
    "- DNN ve Random Forest topluluğu (ensemble) ile ana sınıflandırma yapıldı\n",
    "- Yıldız alt türleri için 3 katmanlı standart model kullanıldı\n",
    "- Döngüsel öğrenme oranı ve trend bazlı erken durdurma stratejileri uygulandı\n",
    "\n",
    "### Sonraki Adımlar\n",
    "- Modelin daha büyük veri setleriyle eğitilmesi\n",
    "- Transfer öğrenme yöntemleriyle performans artırımı\n",
    "- Daha detaylı özellik mühendisliği uygulanması\n",
    "- Web tabanlı bir arayüz ile modelin herkese açık hale getirilmesi\n",
    "\n",
    "Bu notebook'u kullanarak kendi astronomik nesne sınıflandırma projelerinizi gerçekleştirebilirsiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c463e5",
   "metadata": {},
   "source": [
    "## Kendi Verilerinizle Kullanım\n",
    "\n",
    "Kendi verilerinizi yükleyerek tahmin yapmak için aşağıdaki kodu kullanabilirsiniz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55054828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bu kodu kendi verileriniz ile çalıştırmak için düzenleyin\n",
    "def predict_from_csv(csv_path, feature_cols=None):\n",
    "    \"\"\"CSV dosyasından veri yükleyerek tahmin yap\"\"\"\n",
    "    try:\n",
    "        # Veriyi yükle\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Veri yüklendi: {df.shape[0]} satır, {df.shape[1]} sütun\")\n",
    "        \n",
    "        # Özellik sütunlarını seç\n",
    "        if feature_cols is None:\n",
    "            # Varsayılan olarak objid, specobjid, ra, dec ve class dışındaki tüm sütunlar\n",
    "            feature_cols = [col for col in df.columns \n",
    "                           if col not in ['class', 'objid', 'specobjid', 'ra', 'dec', 'subclass']]\n",
    "        \n",
    "        print(f\"Kullanılan özellikler: {feature_cols}\")\n",
    "        X = df[feature_cols].values\n",
    "        \n",
    "        # Verileri ölçeklendir (standart scaler)\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Modelleri yükle\n",
    "        dnn, rf, star_net, le_star, scaler_star, best_w = load_models()\n",
    "        \n",
    "        if dnn is not None:\n",
    "            # Tahmin yap\n",
    "            predictions = full_predict(X_scaled, dnn, rf, star_net, le_star, scaler_star, best_w)\n",
    "            \n",
    "            # Sonuçları DataFrame'e ekle\n",
    "            df['prediction'] = predictions\n",
    "            \n",
    "            # Sonuçları göster\n",
    "            print(\"\\n=== TAHMİN SONUÇLARI ===\")\n",
    "            print(df[['prediction']].head(10))\n",
    "            \n",
    "            # Sonuçları kaydet\n",
    "            output_path = csv_path.replace('.csv', '_predictions.csv')\n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"\\nTüm tahminler kaydedildi: {output_path}\")\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            print(\"Modeller yüklenemediği için tahmin yapılamıyor.\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Tahmin yapılırken hata oluştu: {e}\")\n",
    "        return None\n",
    "\n",
    "# Kendi verilerinizi kullanmak için bu kodu düzenleyin\n",
    "# predict_from_csv('data/your_custom_data.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
